{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1beff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cd246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nexus_test.pipeline.image import ImagePipeline\n",
    "from nexus_test.pipeline.pdf import PDFPipeline\n",
    "from nexus_test.transforms.preprocessing import TessOCRPreprocessor\n",
    "# from nexus_test.transforms.\n",
    "from nexus_test.ocr_engine.pytess import PyTesseractOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5794d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = TessOCRPreprocessor()\n",
    "ocr = PyTesseractOCR()\n",
    "postproc = None\n",
    "img_pipe = ImagePipeline(preproc, ocr, postproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba200e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@ BRITISH\n",
      "@@ COUNCIL\n",
      "\n",
      "Questions 36—40\n",
      "Complete the summary below.\n",
      "\n",
      "Choose NO MORE THAN TWO WORDS from the passage for each answer.\n",
      "\n",
      "Write your answers in boxes 36-40 on your answer sheet.\n",
      "\n",
      "Sobotka argues that big business and users of helium need to help look after helium\n",
      "Stocks because 36 ...........ceeeeeeee will not be encouraged through buying and selling\n",
      "alone. Richardson believes that the 37 .........csesseeeee needs to be withdrawn, as the\n",
      "\n",
      "U.S. provides most of the world's helium. He argues that higher costs would mean\n",
      "\n",
      "people have\n",
      "i}: to use the resource many times over.\n",
      "\n",
      "People should need a 39 .........cseeeeeeeus to access helium that we still have.\n",
      "Furthermore, 2 40 .........cceeeeneeee should ensure that helium is used carefully.\n",
      "\n",
      "14\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "data = img_pipe.forward('/Users/makhmood/PycharmProjects/nexus-test/nexus_test/samples/test_scan.jpg')\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93f6a5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@ BRITISH\n",
      "@@ COUNCIL\n",
      "\n",
      "Questions 36—40\n",
      "Complete the summary below.\n",
      "\n",
      "Choose NO MORE THAN TWO WORDS from the passage for each answer.\n",
      "\n",
      "Write your answers in boxes 36-40 on your answer sheet.\n",
      "\n",
      "Sobotka argues that big business and users of helium need to help look after helium\n",
      "Stocks because 36 ..... will not be encouraged through buying and selling\n",
      "alone. Richardson believes that the 37 ..... needs to be withdrawn, as the\n",
      "\n",
      "U.S. provides most of the world's helium. He argues that higher costs would mean\n",
      "\n",
      "people have\n",
      "i}: to use the resource many times over.\n",
      "\n",
      "People should need a 39 ..... to access helium that we still have.\n",
      "Furthermore, 2 40 ..... should ensure that helium is used carefully.\n",
      "\n",
      "14\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "text = data[0].split('\\n')\n",
    "new_text=[]\n",
    "rx = re.compile(r'(.)\\1{2,}')\n",
    "\n",
    "for line in text:\n",
    "    new_line = []\n",
    "    for word in line.split(' '):\n",
    "        if not rx.search(word):\n",
    "            new_line.append(word)\n",
    "        else:\n",
    "            new_line.append('.....')\n",
    "    new_text.append(' '.join(new_line))\n",
    "for e in new_text:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c983d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.....']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "den_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8602491",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = TessOCRPreprocessor()\n",
    "ocr=PyTesseractOCR()\n",
    "postproc = None\n",
    "pdf_pipe = PDFPipeline(preproc, ocr, postproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1948cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pdf_pipe.forward('/Users/makhmood/PycharmProjects/nexus-test/nexus_test/samples/2003.00744v1_image_pdf.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d563653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Table 1: Performance scores (in %) on test sets. “Acc.” abbreviates accuracy. [de], [%], [@] and [@] denote results reported by Nguyen et al.\n",
      "(2017), Nguyen (2019), Vu et al. (2018) and Vu et al. (2019), respectively. “mBiLSTM” denotes a BiLSTM-based multilingual embedding\n",
      "method. Note that there are higher NLI results reported for XLM-R when fine-tuning on the concatenation of all 15 training datasets in the\n",
      "XNLI corpus. However, those results are not comparable as we only use the Vietnamese monolingual training data for fine-tuning.\n",
      "POS tagging NER NL\n",
      "Model Acc. | Model Fi Model Ace.\n",
      "RDRPOSTagger [Nguyen e7 al., 2014] [&] 95.1 | BiLSTM-CNN-CRF [@] 88.3 | mBiLSTM [Antetxe and Schwenk, 2019] 72.0\n",
      "BiLSTM-CNN-CRF [Ma and Hovy, 2016] [4] | 95.4 | VnCoreNLP-NER [Vu et al., 2018] | 88.6 | multilingual BERT [Wu and Dredze, 2019] | 69.5\n",
      "VnCoreNLP-POS [Nguyen et al., 2017] 95.9 | VNER [Nguyen et al., 2019b] 89.6 | XLMmimstim [Conneau and Lample, 2019] | 76.6\n",
      "jPTDP-v2 [Nguyen and Verspoor, 2018] [4] | 95.7 | BiLSTM-CNN-CRF +ETNLP [@] | 91.1 | XLM-Rigce (Conneau et al., 2019] 15.4\n",
      "jointWPD [Nguyen, 2019] 96.0 | VnCoreNLP-NER + ETNLP [@] 91.3 | XLM-Rurwe [Conneau et al., 2019] 79.7\n",
      "PhoBERTyase 96.7 | PhoBERThuse 93.6 | PhoBERThax 78.5\n",
      "PhoBERT a: 96.8 | PhoBERT ia. 94.7 | PhoBERT,, 80.0\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "than 256 subword tokens are skipped). Following Liu et\n",
      "al. [2019], we optimize the models using Adam [Kingma and\n",
      "Ba, 2014]. We use a batch size of 1024 and a peak learn-\n",
      "ing rate of 0.0004 for PhoBERT ase, and a batch size of 512\n",
      "and a peak learning rate of 0.0002 for PhoBERThaye. We\n",
      "run for 40 epochs (here, the leaming rate is warmed up for\n",
      "2 epochs). We use 4 Nvidia V100 GPUs (16GB each), result-\n",
      "ing in about 540K training steps for PhoBERT pase and 1.08M\n",
      "steps for PhoBERT}ae. We pretrain PhoBERT ase during 3\n",
      "weeks, and then PhoBERTharge during 5 weeks.\n",
      "\n",
      "3 Experiments\n",
      "\n",
      "We evaluate the performance of PhoBERT on three down-\n",
      "stream Vietnamese NLP tasks: POS tagging, NER and NLI.\n",
      "Experimental setup: For the two most common Vietnamese\n",
      "POS tagging and NER tasks, we follow the VnCoreNLP setup\n",
      "[Vu et al., 2018], using standard benchmarks of the VLSP\n",
      "2013 POS tagging dataset and the VLSP 2016 NER dataset\n",
      "[Nguyen et al., 2019a]. For NLI, we use the Vietnamese val-\n",
      "idation and test sets from the XNLI corpus v1.0 [Conneau\n",
      "et al., 2018] where the Vietnamese training data is machine-\n",
      "translated from English. Unlike the 2013 POS tagging and\n",
      "2016 NER datasets which provide the gold word segmenta-\n",
      "tion, for NLI, we use RDRSegmenter to segment the text into\n",
      "words before applying fast BPE to produce subwords from\n",
      "word tokens.\n",
      "\n",
      "Following Devlin et al. [2019], for POS tagging and NER,\n",
      "\n",
      "we append a linear prediction layer on top of the PhoBERT\n",
      "architecture w.r.t. the first subword token of each word. We\n",
      "fine-tune PhoBERT for each task and each dataset indepen-\n",
      "dently, employing the Hugging Face transformers for\n",
      "POS tagging and NER and the RoBERTa implementation in\n",
      "fairseg for NLI. We use AdamW [Loshchilov and Hutter,\n",
      "2019] with a fixed learning rate of 1.e-5 and a batch size of\n",
      "32. We fine-tune in 30 training epochs, evaluate the task per-\n",
      "formance after each epoch on the validation set (here, early\n",
      "stopping is applied when there is no improvement after 5 con-\n",
      "tinuous epochs), and then select the best model to report the\n",
      "final result on the test set.\n",
      "Main results: Table 1 compares our PhoBERT scores with\n",
      "the previous highest reported results, using the same exper-\n",
      "imental setup. PhoBERT helps produce new SOTA results\n",
      "for all the three tasks, where unsurprisingly PhoBERT\\;.¢ ob-\n",
      "tains higher performances than PhoBERT pace.\n",
      "\n",
      "For POS tagging, PhoBERT obtains about 0.8% abso-\n",
      "lute higher accuracy than the feature- and neural network-\n",
      "\n",
      "based models VnCoreNLP-POS (i.e. VaMarMoT) and join-\n",
      "tWPD. For NER, PhoBERTyage is 1.1 points higher F; than\n",
      "PhoBERT},s: which is 2+ points higher than the feature-\n",
      "and neural network-based models VnCoreNLP-NER and\n",
      "BiLSTM-CNN-CRF trained with the BERT-based ETNLP\n",
      "word embeddings [Vu et al., 2019]. For NLI, PhoBERT out-\n",
      "performs the multilingual BERT and the BERT-based cross-\n",
      "lingual model with a new translation language modeling ob-\n",
      "jective XLMyim-+1M by large margins. PhoBERT also per-\n",
      "forms slightly better than the cross-lingual model XLM-R,\n",
      "but using far fewer parameters than XLM-R (base: 135M vs.\n",
      "250M; large: 370M vs. 560M).\n",
      "\n",
      "Discussion: Using more pre-training data can help signifi-\n",
      "cantly improve the quality of the pre-trained language mod-\n",
      "els [Liu e¢ al., 2019]. Thus it is not surprising that PhoBERT\n",
      "helps produce better performance than ETNLP on NER, and\n",
      "the multilingual BERT and XLMyim+tm on NLI (here,\n",
      "PhoBERT employs 20GB of Vietnamese texts while those\n",
      "models employ the 1GB Vietnamese Wikipedia data).\n",
      "\n",
      "Our PhoBERT also does better than XLM-R which uses a\n",
      "2.5TB pre-training corpus containing 137GB of Vietnamese\n",
      "texts (Le. about 137/20 = 7 times bigger than our pre-\n",
      "training corpus). Recall that PhoBERT performs segmenta-\n",
      "tion into subword units after performing a Vietnamese word\n",
      "segmentation, while XLM-R directly applies a BPE method\n",
      "to the syllable-level pre-training Vietnamese data. Clearly,\n",
      "word-level information plays a crucial role for the Viet-\n",
      "namese language understanding task of NLI, i.e. word seg-\n",
      "mentation is necessary to improve the NLI performance. This\n",
      "reconfirms that dedicated language-specific models still out-\n",
      "perform multilingual ones [Martin er al., 2019].\n",
      "\n",
      "Experiments also show that using a straightforward fine-\n",
      "tuning manner as we do can lead to SOTA results. Note that\n",
      "we might boost our downstream task performances even fur-\n",
      "ther by doing a more careful hyper-parameter fine-tuning.\n",
      "\n",
      "4 Conclusion\n",
      "\n",
      "In this paper, we have presented the first public large-scale\n",
      "PhoBERT language models for Vietnamese. We demonstrate\n",
      "the usefulness of PhoBERT by producing new state-of-the-\n",
      "art performances for three Vietnamese NLP tasks of POS\n",
      "tagging, NER and NLI. By publicly releasing PhoBERT, we\n",
      "hope that it can foster future research and applications in Viet-\n",
      "namse NLP. Our PhoBERT and its usage are available at:\n",
      "https://github. com/VinAIResearch/PhoBERT.\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pdf_pipe.forward('/Users/makhmood/PycharmProjects/nexus-test/nexus_test/samples/2003.00744v1_image_pdf.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5397449b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/makhmood/PycharmProjects/nexus-test/nexus_test/samples/2003.00744v1_image_pd.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j3/v2lbykt50tz3lb2m64_xz63m0000gn/T/ipykernel_68618/2285508153.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/makhmood/PycharmProjects/nexus-test/nexus_test/samples/2003.00744v1_image_pd.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File is empty'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/makhmood/PycharmProjects/nexus-test/nexus_test/samples/2003.00744v1_image_pd.pdf'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_path = '/Users/makhmood/PycharmProjects/nexus-test/nexus_test/samples/2003.00744v1_image_pd.pdf'\n",
    "\n",
    "if os.stat(file_path).st_size == 0:\n",
    "    print('File is empty')\n",
    "else:\n",
    "    print('File is not empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd301c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
